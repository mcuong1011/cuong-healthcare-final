{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8719b526",
   "metadata": {},
   "source": [
    "# Healthcare Chatbot Model Training\n",
    "\n",
    "This notebook trains a comprehensive healthcare chatbot with the following capabilities:\n",
    "- **Disease Diagnosis**: Analyze symptoms and provide preliminary diagnosis\n",
    "- **Treatment Recommendations**: Suggest treatment options based on conditions\n",
    "- **Appointment Booking**: Schedule medical appointments with doctors\n",
    "- **Appointment Tracking**: Check existing appointments and medical history\n",
    "- **Health Education**: Provide health tips and preventive care information\n",
    "- **Emergency Detection**: Identify emergency situations and provide immediate guidance\n",
    "\n",
    "## Features:\n",
    "âœ… Multi-intent classification\n",
    "âœ… Symptom-to-disease mapping\n",
    "âœ… Treatment recommendations\n",
    "âœ… Appointment management\n",
    "âœ… Emergency detection\n",
    "âœ… Health education content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c487d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "Training started at: 2025-05-28 21:00:54\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Additional utilities\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"Training started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d40f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    print(\"âœ… NLTK data downloaded successfully!\")\n",
    "except:\n",
    "    print(\"âš ï¸ NLTK data download failed, but continuing...\")\n",
    "\n",
    "# Initialize preprocessing tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['xin', 'chÃ o', 'tÃ´i', 'báº¡n', 'cá»§a', 'vÃ ', 'cÃ³', 'lÃ ', 'khÃ´ng', 'vá»›i', 'cho', 'Ä‘á»ƒ'])\n",
    "\n",
    "print(\"âœ… Text preprocessing tools initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69dd21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load healthcare knowledge base\n",
    "with open('healthcare_knowledge_base.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract patterns and labels for training\n",
    "patterns = []\n",
    "labels = []\n",
    "intent_responses = {}\n",
    "\n",
    "for intent in data['intents']:\n",
    "    tag = intent['tag']\n",
    "    intent_responses[tag] = intent['responses']\n",
    "    \n",
    "    for pattern in intent['patterns']:\n",
    "        patterns.append(pattern)\n",
    "        labels.append(tag)\n",
    "\n",
    "print(f\"ğŸ“Š Dataset Statistics:\")\n",
    "print(f\"   Total patterns: {len(patterns)}\")\n",
    "print(f\"   Total intents: {len(set(labels))}\")\n",
    "print(f\"   Intent distribution:\")\n",
    "\n",
    "# Show intent distribution\n",
    "label_counts = pd.Series(labels).value_counts()\n",
    "for intent, count in label_counts.items():\n",
    "    print(f\"     {intent}: {count} patterns\")\n",
    "\n",
    "print(\"\\nâœ… Knowledge base loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing for Vietnamese and English\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and short words\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    # Lemmatize (for English words)\n",
    "    try:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test preprocessing\n",
    "test_text = \"TÃ´i bá»‹ sá»‘t cao vÃ  Ä‘au Ä‘áº§u, cáº§n khÃ¡m bÃ¡c sÄ©\"\n",
    "processed = preprocess_text(test_text)\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Processed: {processed}\")\n",
    "\n",
    "# Preprocess all patterns\n",
    "processed_patterns = [preprocess_text(pattern) for pattern in patterns]\n",
    "print(f\"\\nâœ… Text preprocessing completed!\")\n",
    "print(f\"Sample processed patterns:\")\n",
    "for i in range(min(3, len(processed_patterns))):\n",
    "    print(f\"   {i+1}. {processed_patterns[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b4000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    processed_patterns, labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Data Split:\")\n",
    "print(f\"   Training samples: {len(X_train)}\")\n",
    "print(f\"   Testing samples: {len(X_test)}\")\n",
    "\n",
    "# Create TF-IDF vectorizer with optimized parameters\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    ngram_range=(1, 2),  # Include bigrams\n",
    "    min_df=1,\n",
    "    max_df=0.9,\n",
    "    stop_words=None  # We already removed stopwords\n",
    ")\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"\\nâœ… TF-IDF features created!\")\n",
    "print(f\"   Feature matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"   Vocabulary size: {len(vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a36a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models for comparison\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(alpha=0.1),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "model_scores = {}\n",
    "best_model = None\n",
    "best_score = 0\n",
    "\n",
    "print(\"ğŸš€ Training multiple models...\")\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n   Training {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train_tfidf, y_train, cv=3, scoring='accuracy')\n",
    "    avg_cv_score = np.mean(cv_scores)\n",
    "    \n",
    "    # Test accuracy\n",
    "    test_pred = model.predict(X_test_tfidf)\n",
    "    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "    \n",
    "    model_scores[name] = {\n",
    "        'cv_score': avg_cv_score,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"     Cross-validation score: {avg_cv_score:.4f}\")\n",
    "    print(f\"     Test accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Track best model\n",
    "    if avg_cv_score > best_score:\n",
    "        best_score = avg_cv_score\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "\n",
    "print(f\"\\nğŸ† Best Model: {best_model_name} (CV Score: {best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation of best model\n",
    "print(f\"\\nğŸ“ˆ Detailed Evaluation - {best_model_name}:\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_model.predict(X_test_tfidf)\n",
    "y_pred_proba = best_model.predict_proba(X_test_tfidf)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "labels_unique = sorted(set(labels))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels_unique, yticklabels=labels_unique)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    importances = best_model.feature_importances_\n",
    "    \n",
    "    # Get top features\n",
    "    indices = np.argsort(importances)[::-1][:20]\n",
    "    top_features = [(feature_names[i], importances[i]) for i in indices]\n",
    "    \n",
    "    print(\"\\nTop 10 Important Features:\")\n",
    "    for i, (feature, importance) in enumerate(top_features[:10]):\n",
    "        print(f\"   {i+1}. {feature}: {importance:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Model evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f64cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Healthcare Chatbot Class with Advanced Features\n",
    "class HealthcareChatbot:\n",
    "    def __init__(self, model, vectorizer, intent_responses):\n",
    "        self.model = model\n",
    "        self.vectorizer = vectorizer\n",
    "        self.intent_responses = intent_responses\n",
    "        self.conversation_history = []\n",
    "        self.user_context = {}\n",
    "        \n",
    "    def preprocess_input(self, text):\n",
    "        \"\"\"Preprocess user input\"\"\"\n",
    "        return preprocess_text(text)\n",
    "    \n",
    "    def predict_intent(self, text):\n",
    "        \"\"\"Predict intent with confidence score\"\"\"\n",
    "        processed_text = self.preprocess_input(text)\n",
    "        text_tfidf = self.vectorizer.transform([processed_text])\n",
    "        \n",
    "        # Get prediction and probabilities\n",
    "        intent = self.model.predict(text_tfidf)[0]\n",
    "        probabilities = self.model.predict_proba(text_tfidf)[0]\n",
    "        confidence = max(probabilities)\n",
    "        \n",
    "        return intent, confidence\n",
    "    \n",
    "    def get_response(self, user_input):\n",
    "        \"\"\"Generate response based on user input\"\"\"\n",
    "        intent, confidence = self.predict_intent(user_input)\n",
    "        \n",
    "        # Log conversation\n",
    "        self.conversation_history.append({\n",
    "            'user_input': user_input,\n",
    "            'predicted_intent': intent,\n",
    "            'confidence': confidence,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Handle low confidence\n",
    "        if confidence < 0.5:\n",
    "            return {\n",
    "                'intent': 'unknown',\n",
    "                'confidence': confidence,\n",
    "                'response': \"Xin lá»—i, tÃ´i khÃ´ng hiá»ƒu rÃµ Ã½ báº¡n. Báº¡n cÃ³ thá»ƒ nÃ³i rÃµ hÆ¡n vá» triá»‡u chá»©ng hoáº·c váº¥n Ä‘á» sá»©c khá»e cá»§a mÃ¬nh khÃ´ng? Hoáº·c báº¡n cÃ³ thá»ƒ há»i vá»: cháº©n Ä‘oÃ¡n triá»‡u chá»©ng, Ä‘áº·t lá»‹ch khÃ¡m, tÆ° váº¥n sá»©c khá»e.\",\n",
    "                'suggestions': ['TÃ´i bá»‹ Ä‘au Ä‘áº§u', 'Äáº·t lá»‹ch khÃ¡m bá»‡nh', 'TÆ° váº¥n sá»©c khá»e']\n",
    "            }\n",
    "        \n",
    "        # Get response for intent\n",
    "        responses = self.intent_responses.get(intent, [\"Xin lá»—i, tÃ´i chÆ°a Ä‘Æ°á»£c train Ä‘á»ƒ xá»­ lÃ½ cÃ¢u há»i nÃ y.\"])\n",
    "        response = np.random.choice(responses)\n",
    "        \n",
    "        # Special handling for appointment booking\n",
    "        if intent == 'appointment_booking':\n",
    "            return self.handle_appointment_booking(response)\n",
    "        \n",
    "        # Special handling for emergency\n",
    "        elif intent == 'emergency':\n",
    "            return self.handle_emergency(response)\n",
    "        \n",
    "        # Special handling for symptom diagnosis\n",
    "        elif intent in ['fever_symptoms', 'headache_symptoms', 'cough_symptoms', 'stomach_pain']:\n",
    "            return self.handle_symptom_diagnosis(intent, response)\n",
    "        \n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'confidence': confidence,\n",
    "            'response': response,\n",
    "            'additional_info': self.get_additional_info(intent)\n",
    "        }\n",
    "    \n",
    "    def handle_appointment_booking(self, base_response):\n",
    "        \"\"\"Handle appointment booking with interactive flow\"\"\"\n",
    "        return {\n",
    "            'intent': 'appointment_booking',\n",
    "            'response': base_response,\n",
    "            'action': 'collect_appointment_info',\n",
    "            'required_fields': ['name', 'phone', 'date_of_birth', 'symptoms', 'department', 'preferred_time'],\n",
    "            'next_step': 'HÃ£y báº¯t Ä‘áº§u báº±ng cÃ¡ch cho tÃ´i biáº¿t há» tÃªn Ä‘áº§y Ä‘á»§ cá»§a báº¡n?'\n",
    "        }\n",
    "    \n",
    "    def handle_emergency(self, base_response):\n",
    "        \"\"\"Handle emergency situations with immediate action\"\"\"\n",
    "        return {\n",
    "            'intent': 'emergency',\n",
    "            'response': base_response,\n",
    "            'priority': 'URGENT',\n",
    "            'action': 'emergency_protocol',\n",
    "            'immediate_steps': [\n",
    "                'Gá»i cáº¥p cá»©u 115 ngay láº­p tá»©c',\n",
    "                'Giá»¯ bÃ¬nh tÄ©nh vÃ  khÃ´ng di chuyá»ƒn',\n",
    "                'Chuáº©n bá»‹ thÃ´ng tin cÆ¡ báº£n: tuá»•i, triá»‡u chá»©ng, thá»i gian báº¯t Ä‘áº§u'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def handle_symptom_diagnosis(self, intent, base_response):\n",
    "        \"\"\"Enhanced symptom diagnosis with follow-up questions\"\"\"\n",
    "        follow_up_questions = {\n",
    "            'fever_symptoms': [\n",
    "                'Nhiá»‡t Ä‘á»™ cÆ¡ thá»ƒ hiá»‡n táº¡i cá»§a báº¡n lÃ  bao nhiÃªu?',\n",
    "                'Báº¡n cÃ³ kÃ¨m theo triá»‡u chá»©ng nÃ o khÃ¡c khÃ´ng? (Ä‘au Ä‘áº§u, ho, Ä‘au há»ng)',\n",
    "                'Triá»‡u chá»©ng Ä‘Ã£ kÃ©o dÃ i bao lÃ¢u?'\n",
    "            ],\n",
    "            'headache_symptoms': [\n",
    "                'Äau Ä‘áº§u á»Ÿ vá»‹ trÃ­ nÃ o? (trÃ¡n, thÃ¡i dÆ°Æ¡ng, gÃ¡y)',\n",
    "                'Má»©c Ä‘á»™ Ä‘au tá»« 1-10 lÃ  bao nhiÃªu?',\n",
    "                'CÃ³ kÃ¨m buá»“n nÃ´n hoáº·c nháº¡y cáº£m vá»›i Ã¡nh sÃ¡ng khÃ´ng?'\n",
    "            ],\n",
    "            'cough_symptoms': [\n",
    "                'Ho khan hay cÃ³ Ä‘á»m?',\n",
    "                'Ho nhiá»u vÃ o thá»i gian nÃ o trong ngÃ y?',\n",
    "                'CÃ³ kÃ¨m sá»‘t hoáº·c khÃ³ thá»Ÿ khÃ´ng?'\n",
    "            ],\n",
    "            'stomach_pain': [\n",
    "                'Äau bá»¥ng á»Ÿ vá»‹ trÃ­ nÃ o cá»¥ thá»ƒ?',\n",
    "                'Má»©c Ä‘á»™ Ä‘au vÃ  cÃ³ Ä‘au liÃªn tá»¥c khÃ´ng?',\n",
    "                'CÃ³ kÃ¨m buá»“n nÃ´n, tiÃªu cháº£y hoáº·c tÃ¡o bÃ³n khÃ´ng?'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'response': base_response,\n",
    "            'follow_up_questions': follow_up_questions.get(intent, []),\n",
    "            'recommended_action': 'Náº¿u triá»‡u chá»©ng nghiÃªm trá»ng hoáº·c kÃ©o dÃ i, hÃ£y Ä‘áº·t lá»‹ch khÃ¡m bÃ¡c sÄ©.',\n",
    "            'appointment_suggestion': True\n",
    "        }\n",
    "    \n",
    "    def get_additional_info(self, intent):\n",
    "        \"\"\"Provide additional helpful information based on intent\"\"\"\n",
    "        additional_info = {\n",
    "            'greeting': {\n",
    "                'services': ['Cháº©n Ä‘oÃ¡n triá»‡u chá»©ng', 'Äáº·t lá»‹ch khÃ¡m', 'TÆ° váº¥n sá»©c khá»e', 'ThÃ´ng tin thuá»‘c'],\n",
    "                'emergency_note': 'Trong trÆ°á»ng há»£p kháº©n cáº¥p, hÃ£y gá»i 115'\n",
    "            },\n",
    "            'health_tips': {\n",
    "                'related_topics': ['Dinh dÆ°á»¡ng', 'Táº­p thá»ƒ dá»¥c', 'Giáº¥c ngá»§', 'Quáº£n lÃ½ stress'],\n",
    "                'check_up_reminder': 'Nhá»› khÃ¡m sá»©c khá»e Ä‘á»‹nh ká»³ 6 thÃ¡ng/láº§n'\n",
    "            },\n",
    "            'medicine_info': {\n",
    "                'safety_warning': 'LuÃ´n tham kháº£o bÃ¡c sÄ© hoáº·c dÆ°á»£c sÄ© trÆ°á»›c khi dÃ¹ng thuá»‘c',\n",
    "                'pharmacy_hotline': 'Hotline dÆ°á»£c sÄ©: 1900-xxxx'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return additional_info.get(intent, {})\n",
    "    \n",
    "    def get_conversation_summary(self):\n",
    "        \"\"\"Get conversation history summary\"\"\"\n",
    "        if not self.conversation_history:\n",
    "            return \"ChÆ°a cÃ³ cuá»™c trÃ² chuyá»‡n nÃ o.\"\n",
    "        \n",
    "        intents = [conv['predicted_intent'] for conv in self.conversation_history]\n",
    "        intent_counts = pd.Series(intents).value_counts().to_dict()\n",
    "        \n",
    "        return {\n",
    "            'total_interactions': len(self.conversation_history),\n",
    "            'intent_distribution': intent_counts,\n",
    "            'last_interaction': self.conversation_history[-1]['timestamp']\n",
    "        }\n",
    "\n",
    "print(\"âœ… Healthcare Chatbot class created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42157b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Healthcare Chatbot\n",
    "chatbot = HealthcareChatbot(best_model, vectorizer, intent_responses)\n",
    "\n",
    "print(\"ğŸ¤– Healthcare Chatbot initialized successfully!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"             CHATBOT TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test conversations\n",
    "test_inputs = [\n",
    "    \"Xin chÃ o, tÃ´i cáº§n tÆ° váº¥n sá»©c khá»e\",\n",
    "    \"TÃ´i bá»‹ sá»‘t cao vÃ  Ä‘au Ä‘áº§u tá»« 2 ngÃ y nay\",\n",
    "    \"LÃ m tháº¿ nÃ o Ä‘á»ƒ Ä‘áº·t lá»‹ch khÃ¡m bÃ¡c sÄ©?\",\n",
    "    \"TÃ´i ho khan kÃ©o dÃ i, cÃ³ Ä‘á»m mÃ u vÃ ng\",\n",
    "    \"Äau bá»¥ng dÆ°á»›i bÃªn pháº£i ráº¥t dá»¯ dá»™i\",\n",
    "    \"Cho tÃ´i má»™t sá»‘ lá»i khuyÃªn Ä‘á»ƒ sá»‘ng khá»e máº¡nh\",\n",
    "    \"Cáº£m Æ¡n báº¡n ráº¥t nhiá»u\"\n",
    "]\n",
    "\n",
    "for i, user_input in enumerate(test_inputs, 1):\n",
    "    print(f\"\\n[Cuá»™c há»™i thoáº¡i {i}]\")\n",
    "    print(f\"ğŸ‘¤ NgÆ°á»i dÃ¹ng: {user_input}\")\n",
    "    \n",
    "    response = chatbot.get_response(user_input)\n",
    "    \n",
    "    print(f\"ğŸ¤– Chatbot: {response['response']}\")\n",
    "    print(f\"ğŸ“Š Intent: {response['intent']} (Confidence: {response.get('confidence', 0):.2f})\")\n",
    "    \n",
    "    # Show additional information if available\n",
    "    if 'follow_up_questions' in response and response['follow_up_questions']:\n",
    "        print(\"â“ CÃ¢u há»i thÃªm:\")\n",
    "        for q in response['follow_up_questions'][:2]:  # Show first 2 questions\n",
    "            print(f\"   â€¢ {q}\")\n",
    "    \n",
    "    if 'immediate_steps' in response:\n",
    "        print(\"ğŸš¨ CÃ¡c bÆ°á»›c kháº©n cáº¥p:\")\n",
    "        for step in response['immediate_steps']:\n",
    "            print(f\"   â€¢ {step}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Show conversation summary\n",
    "print(\"\\nğŸ“ˆ Conversation Summary:\")\n",
    "summary = chatbot.get_conversation_summary()\n",
    "print(f\"   Total interactions: {summary['total_interactions']}\")\n",
    "print(f\"   Intent distribution: {summary['intent_distribution']}\")\n",
    "\n",
    "print(\"\\nâœ… Chatbot testing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e55a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and components\n",
    "print(\"ğŸ’¾ Saving trained model and components...\")\n",
    "\n",
    "# Create model package\n",
    "model_package = {\n",
    "    'model': best_model,\n",
    "    'vectorizer': vectorizer,\n",
    "    'intent_responses': intent_responses,\n",
    "    'model_name': best_model_name,\n",
    "    'training_accuracy': best_score,\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'features_count': len(vectorizer.vocabulary_),\n",
    "    'intents_count': len(set(labels))\n",
    "}\n",
    "\n",
    "# Save with pickle\n",
    "with open('healthcare_chatbot_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "print(\"âœ… Model saved as 'healthcare_chatbot_model.pkl'\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_type': best_model_name,\n",
    "    'training_accuracy': float(best_score),\n",
    "    'test_accuracy': float(model_scores[best_model_name]['test_accuracy']),\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'total_patterns': len(patterns),\n",
    "    'total_intents': len(set(labels)),\n",
    "    'features_count': len(vectorizer.vocabulary_),\n",
    "    'model_parameters': str(best_model.get_params()),\n",
    "    'intent_list': list(set(labels))\n",
    "}\n",
    "\n",
    "with open('model_metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… Model metadata saved as 'model_metadata.json'\")\n",
    "\n",
    "# Create simple deployment script\n",
    "deployment_script = '''\n",
    "#!/usr/bin/env python3\n",
    "# Healthcare Chatbot Deployment Script\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class HealthcareChatbotLoader:\n",
    "    def __init__(self, model_path='healthcare_chatbot_model.pkl'):\n",
    "        \"\"\"Load trained healthcare chatbot model\"\"\"\n",
    "        with open(model_path, 'rb') as f:\n",
    "            self.model_package = pickle.load(f)\n",
    "        \n",
    "        self.model = self.model_package['model']\n",
    "        self.vectorizer = self.model_package['vectorizer']\n",
    "        self.intent_responses = self.model_package['intent_responses']\n",
    "        \n",
    "        print(f\"âœ… Healthcare Chatbot loaded successfully!\")\n",
    "        print(f\"   Model: {self.model_package['model_name']}\")\n",
    "        print(f\"   Training Accuracy: {self.model_package['training_accuracy']:.2f}\")\n",
    "        print(f\"   Trained on: {self.model_package['training_date']}\")\n",
    "    \n",
    "    def predict(self, user_input):\n",
    "        \"\"\"Make prediction for user input\"\"\"\n",
    "        # Preprocess text (simplified version)\n",
    "        processed_text = user_input.lower()\n",
    "        text_tfidf = self.vectorizer.transform([processed_text])\n",
    "        \n",
    "        # Predict\n",
    "        intent = self.model.predict(text_tfidf)[0]\n",
    "        confidence = max(self.model.predict_proba(text_tfidf)[0])\n",
    "        \n",
    "        # Get response\n",
    "        responses = self.intent_responses.get(intent, [\"Sorry, I don't understand.\"])\n",
    "        response = responses[0]  # Take first response\n",
    "        \n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'confidence': float(confidence),\n",
    "            'response': response,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load chatbot\n",
    "    chatbot = HealthcareChatbotLoader()\n",
    "    \n",
    "    # Test\n",
    "    test_inputs = [\n",
    "        \"TÃ´i bá»‹ sá»‘t cao\",\n",
    "        \"Äáº·t lá»‹ch khÃ¡m bá»‡nh\",\n",
    "        \"Cáº£m Æ¡n báº¡n\"\n",
    "    ]\n",
    "    \n",
    "    for user_input in test_inputs:\n",
    "        result = chatbot.predict(user_input)\n",
    "        print(f\"Input: {user_input}\")\n",
    "        print(f\"Intent: {result['intent']} (Confidence: {result['confidence']:.2f})\")\n",
    "        print(f\"Response: {result['response'][:100]}...\")\n",
    "        print(\"-\" * 50)\n",
    "'''\n",
    "\n",
    "with open('deploy_chatbot.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(deployment_script)\n",
    "\n",
    "print(\"âœ… Deployment script saved as 'deploy_chatbot.py'\")\n",
    "\n",
    "print(\"\\nğŸ‰ MODEL TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"\\nğŸ“‹ Summary:\")\n",
    "print(f\"   ğŸ† Best Model: {best_model_name}\")\n",
    "print(f\"   ğŸ“Š Training Accuracy: {best_score:.4f}\")\n",
    "print(f\"   ğŸ“Š Test Accuracy: {model_scores[best_model_name]['test_accuracy']:.4f}\")\n",
    "print(f\"   ğŸ“¦ Total Patterns: {len(patterns)}\")\n",
    "print(f\"   ğŸ¯ Total Intents: {len(set(labels))}\")\n",
    "print(f\"   ğŸ”¤ Vocabulary Size: {len(vectorizer.vocabulary_)}\")\n",
    "print(f\"\\nğŸ“ Generated Files:\")\n",
    "print(f\"   â€¢ healthcare_chatbot_model.pkl - Trained model\")\n",
    "print(f\"   â€¢ model_metadata.json - Model information\")\n",
    "print(f\"   â€¢ deploy_chatbot.py - Deployment script\")\n",
    "print(f\"\\nğŸš€ Ready for integration with your healthcare system!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00222c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive healthcare knowledge base\n",
    "healthcare_knowledge_base = {\n",
    "    \"intents\": [\n",
    "        {\n",
    "            \"tag\": \"greeting\",\n",
    "            \"patterns\": [\n",
    "                \"xin chÃ o\", \"chÃ o báº¡n\", \"hello\", \"hi\", \"chÃ o\", \"xin chÃ o bÃ¡c sÄ©\",\n",
    "                \"chÃ o buá»•i sÃ¡ng\", \"chÃ o buá»•i chiá»u\", \"chÃ o buá»•i tá»‘i\", \"báº¡n cÃ³ khá»e khÃ´ng\",\n",
    "                \"chÃ o em\", \"chÃ o anh\", \"chÃ o chá»‹\", \"xin chÃ o!\", \"hi there\", \"hey\"\n",
    "            ],\n",
    "            \"responses\": [\n",
    "                \"Xin chÃ o! TÃ´i lÃ  trá»£ lÃ½ y táº¿ AI. TÃ´i cÃ³ thá»ƒ giÃºp báº¡n:\n",
    "âœ… TÆ° váº¥n sÆ¡ bá»™ vá» triá»‡u chá»©ng\n",
    "âœ… Äáº·t lá»‹ch khÃ¡m bá»‡nh\n",
    "âœ… Tra cá»©u lá»‹ch háº¹n\n",
    "âœ… Cung cáº¥p thÃ´ng tin y táº¿\n",
    "Báº¡n cáº§n há»— trá»£ gÃ¬?\",\n",
    "                \"ChÃ o báº¡n! TÃ´i lÃ  chatbot y táº¿ thÃ´ng minh. HÃ£y mÃ´ táº£ triá»‡u chá»©ng hoáº·c nÃ³i 'Ä‘áº·t lá»‹ch khÃ¡m' Ä‘á»ƒ tÃ´i há»— trá»£ báº¡n.\",\n",
    "                \"Xin chÃ o! TÃ´i sáºµn sÃ ng há»— trá»£ báº¡n vá» cÃ¡c váº¥n Ä‘á» sá»©c khá»e. Báº¡n cÃ³ triá»‡u chá»©ng gÃ¬ cáº§n tÆ° váº¥n khÃ´ng?\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"tag\": \"goodbye\",\n",
    "            \"patterns\": [\n",
    "                \"táº¡m biá»‡t\", \"bye\", \"goodbye\", \"see you\", \"chÃ o táº¡m biá»‡t\", \"háº¹n gáº·p láº¡i\",\n",
    "                \"cáº£m Æ¡n vÃ  táº¡m biá»‡t\", \"bye bye\", \"chÃºc ngá»§ ngon\", \"háº¹n gáº·p láº¡i\", \"táº¡m biá»‡t nhÃ©\"\n",
    "            ],\n",
    "            \"responses\": [\n",
    "                \"Táº¡m biá»‡t! ChÃºc báº¡n sá»©c khá»e tá»‘t. HÃ£y Ä‘áº¿n bÃ¡c sÄ© náº¿u triá»‡u chá»©ng khÃ´ng cáº£i thiá»‡n.\",\n",
    "                \"ChÃ o táº¡m biá»‡t! LuÃ´n chÄƒm sÃ³c sá»©c khá»e báº£n thÃ¢n nhÃ©.\",\n",
    "                \"Háº¹n gáº·p láº¡i! Nhá»› uá»‘ng Ä‘á»§ nÆ°á»›c vÃ  nghá»‰ ngÆ¡i há»£p lÃ½.\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"tag\": \"thanks\",\n",
    "            \"patterns\": [\n",
    "                \"cáº£m Æ¡n\", \"thank you\", \"thanks\", \"cáº£m Æ¡n báº¡n\", \"cáº£m Æ¡n nhiá»u\",\n",
    "                \"cáº£m Æ¡n bÃ¡c sÄ©\", \"thanks a lot\", \"cáº£m Æ¡n ráº¥t nhiá»u\", \"tÃ´i cáº£m Æ¡n\"\n",
    "            ],\n",
    "            \"responses\": [\n",
    "                \"KhÃ´ng cÃ³ gÃ¬! TÃ´i luÃ´n sáºµn sÃ ng há»— trá»£ báº¡n vá» cÃ¡c váº¥n Ä‘á» sá»©c khá»e.\",\n",
    "                \"Ráº¥t vui Ä‘Æ°á»£c giÃºp Ä‘á»¡ báº¡n! Nhá»› tÃ¡i khÃ¡m náº¿u cáº§n thiáº¿t.\",\n",
    "                \"ÄÃ³ lÃ  nhiá»‡m vá»¥ cá»§a tÃ´i! ChÃºc báº¡n mau khá»i bá»‡nh.\"\n",
    "            ]\n",
    "        },\n",
    "        # DISEASE DIAGNOSIS INTENTS\n",
    "        {\n",
    "            \"tag\": \"symptom_fever\",\n",
    "            \"patterns\": [\n",
    "                \"tÃ´i bá»‹ sá»‘t\", \"sá»‘t cao\", \"nhiá»‡t Ä‘á»™ cÆ¡ thá»ƒ cao\", \"bá»‹ nÃ³ng ngÆ°á»i\", \"sá»‘t vÃ  á»›n láº¡nh\",\n",
    "                \"sá»‘t xuáº¥t huyáº¿t\", \"sá»‘t virus\", \"sá»‘t vi khuáº©n\", \"sá»‘t 39 Ä‘á»™\", \"sá»‘t 40 Ä‘á»™\",\n",
    "                \"fever\", \"high temperature\", \"feeling hot\", \"burning up\", \"chills and fever\"\n",
    "            ],\n",
    "            \"responses\": [\n",
    "                \"ğŸŒ¡ï¸ **TRIá»†U CHá»¨NG: Sá»T**\\n\\n**NguyÃªn nhÃ¢n cÃ³ thá»ƒ:**\\n- Nhiá»…m virus (cáº£m, cÃºm)\\n- Nhiá»…m khuáº©n\\n- Sá»‘t xuáº¥t huyáº¿t\\n- ViÃªm nhiá»…m\\n\\n**Cáº§n lÃ m ngay:**\\nâœ… Äo nhiá»‡t Ä‘á»™ chÃ­nh xÃ¡c\\nâœ… Uá»‘ng nhiá»u nÆ°á»›c\\nâœ… Nghá»‰ ngÆ¡i\\nâœ… DÃ¹ng thuá»‘c háº¡ sá»‘t (Paracetamol)\\n\\nâš ï¸ **Äáº¾N Bá»†NH VIá»†N NGAY** náº¿u:\\n- Sá»‘t trÃªn 39Â°C\\n- Sá»‘t kÃ¨m khÃ³ thá»Ÿ\\n- Sá»‘t kÃ¨m phÃ¡t ban\\n- Sá»‘t kÃ©o dÃ i > 3 ngÃ y\\n\\nBáº¡n cÃ³ muá»‘n Ä‘áº·t lá»‹ch khÃ¡m khÃ´ng?\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"tag\": \"symptom_headache\",\n",
    "            \"patterns\": [\n",
    "                \"Ä‘au Ä‘áº§u\", \"nhá»©c Ä‘áº§u\", \"Ä‘au ná»­a Ä‘áº§u\", \"migraine\", \"headache\", \"Ä‘au Ä‘áº§u dá»¯ dá»™i\",\n",
    "                \"chÃ³ng máº·t\", \"Ä‘au Ä‘áº§u kÃ¨m buá»“n nÃ´n\", \"Ä‘au thÃ¡i dÆ°Æ¡ng\", \"Ä‘au Ä‘áº§u mÃ£n tÃ­nh\",\n",
    "                \"tension headache\", \"cluster headache\", \"Ä‘au Ä‘áº§u cÄƒng tháº³ng\"\n",
    "            ],\n",
    "            \"responses\": [\n",
    "                \"ğŸ§  **TRIá»†U CHá»¨NG: ÄAU Äáº¦U**\\n\\n**PhÃ¢n loáº¡i:**\\n- Äau Ä‘áº§u cÄƒng tháº³ng (phá»• biáº¿n nháº¥t)\\n- Migraine (Ä‘au ná»­a Ä‘áº§u)\\n- Äau Ä‘áº§u do sinus\\n- Äau Ä‘áº§u cluster\\n\\n**Äiá»u trá»‹ táº¡i nhÃ :**\\nâœ… Nghá»‰ ngÆ¡i trong phÃ²ng tá»‘i\\nâœ… ChÆ°á»m láº¡nh lÃªn trÃ¡n\\nâœ… Massage nháº¹ thÃ¡i dÆ°Æ¡ng\\nâœ… Uá»‘ng Ä‘á»§ nÆ°á»›c\\nâœ… DÃ¹ng thuá»‘c giáº£m Ä‘au (Ibuprofen, Paracetamol)\\n\\nâš ï¸ **KHáº¨N Cáº¤P** náº¿u:\\n- Äau Ä‘áº§u Ä‘á»™t ngá»™t, dá»¯ dá»™i\\n- KÃ¨m sá»‘t cao, cá»©ng gÃ¡y\\n- KÃ¨m rá»‘i loáº¡n thá»‹ giÃ¡c\\n- KÃ¨m yáº¿u liá»‡t tay chÃ¢n\\n\\nBáº¡n muá»‘n Ä‘áº·t lá»‹ch khÃ¡m tháº§n kinh khÃ´ng?\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"tag\": \"symptom_cough\",\n",
    "            \"patterns\": [\n",
    "                \"ho\", \"ho khan\", \"ho cÃ³ Ä‘á»m\", \"ho ra mÃ¡u\", \"cough\", \"ho kÃ©o dÃ i\",\n",
    "                \"ho Ä‘Ãªm\", \"ho sáº·c\", \"ho khÃ² khÃ¨\", \"ho cÃ³ Ä‘á»m vÃ ng\", \"ho cÃ³ Ä‘á»m xanh\",\n",
    "                \"persistent cough\", \"dry cough\", \"wet cough\", \"whooping cough\"\n",
    "            ],\n",
    "            \"responses\": [\n",
    "                \"ğŸ« **TRIá»†U CHá»¨NG: HO**\\n\\n**PhÃ¢n loáº¡i:**\\n- Ho khan: virus, dá»‹ á»©ng, GERD\\n- Ho cÃ³ Ä‘á»m: nhiá»…m khuáº©n, viÃªm phá»•i\\n- Ho ra mÃ¡u: TB, ung thÆ° phá»•i, nhiá»…m trÃ¹ng\\n\\n**Äiá»u trá»‹:**\\nâœ… Uá»‘ng nhiá»u nÆ°á»›c áº¥m\\nâœ… SÃºc há»ng nÆ°á»›c muá»‘i\\nâœ… DÃ¹ng máº­t ong chanh\\nâœ… HÃ­t hÆ¡i nÆ°á»›c áº¥m\\nâœ… Thuá»‘c ho (Dextromethorphan)\\n\\nâš ï¸ **Äáº¾N Bá»†NH VIá»†N NGAY** náº¿u:\\n- Ho ra mÃ¡u\\n- KhÃ³ thá»Ÿ\\n- Sá»‘t cao kÃ¨m ho\\n- Ho > 2 tuáº§n\\n- Äau ngá»±c khi ho\\n\\nCáº§n Ä‘áº·t lá»‹ch khÃ¡m phá»•i khÃ´ng?\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"tag\": \"symptom_stomach_pain\",\n",
    "            \"patterns\": [\n",
    "                \"Ä‘au bá»¥ng\", \"Ä‘au dáº¡ dÃ y\", \"Ä‘au ruá»™t\", \"stomach pain\", \"abdominal pain\",\n",
    "                \"Ä‘au bá»¥ng dÆ°á»›i\", \"Ä‘au bá»¥ng trÃªn\", \"Ä‘au quáº·n bá»¥ng\", \"Ä‘au bá»¥ng kÃ¨m nÃ´n\",\n",
    "                \"gastritis\", \"peptic ulcer\", \"appendicitis\", \"viÃªm ruá»™t thá»«a\", \"viÃªm dáº¡ dÃ y\"\n",
    "            ],\n",
    "            \"responses\": [\n",
    "                \"ğŸ¤¢ **TRIá»†U CHá»¨NG: ÄAU Bá»¤NG**\\n\\n**Vá»‹ trÃ­ & NguyÃªn nhÃ¢n:**\\n- Äau thÆ°á»£ng vá»‹: dáº¡ dÃ y, tÃ¡ trÃ ng\\n- Äau háº¡ sÆ°á»n pháº£i: gan, tÃºi máº­t\\n- Äau há»‘ cháº­u pháº£i: ruá»™t thá»«a\\n- Äau toÃ n bá»¥ng: viÃªm phÃºc máº¡c\\n\\n**Äiá»u trá»‹:**\\nâœ… Nhá»‹n Äƒn táº¡m thá»i\\nâœ… Uá»‘ng nÆ°á»›c nhá» giá»t\\nâœ… Náº±m nghá»‰, nÃ©n áº¥m\\nâœ… TrÃ¡nh thuá»‘c giáº£m Ä‘au\\n\\nğŸš¨ **KHáº¨N Cáº¤P** khi:\\n- Äau dá»¯ dá»™i Ä‘á»™t ngá»™t\\n- Cá»©ng bá»¥ng\\n- NÃ´n ra mÃ¡u\\n- Sá»‘t cao kÃ¨m Ä‘au bá»¥ng\\n- Äáº¡i tiá»‡n ra mÃ¡u\\n\\nCáº§n khÃ¡m tiÃªu hÃ³a ngay khÃ´ng?\"\n",
    "            ]\n",
    "        },\n",
    "        # APPOINTMENT BOOKING\n",
    "        {\n",
    "            \"tag\": \"appointment_booking\",\n",
    "            \"patterns\": [\n",
    "                \"Ä‘áº·t lá»‹ch khÃ¡m\", \"book appointment\", \"schedule appointment\", \"Ä‘áº·t háº¹n\",\n",
    "                \"muá»‘n khÃ¡m bá»‡nh\", \"Ä‘Äƒng kÃ½ khÃ¡m\", \"tÃ´i muá»‘n Ä‘áº·t lá»‹ch\", \"lÃ m sao Ä‘á»ƒ Ä‘áº·t lá»‹ch\",\n",
    "                \"Ä‘áº·t lá»‹ch háº¹n\", \"make appointment\", \"want to see doctor\", \"cáº§n gáº·p bÃ¡c sÄ©\"\n",
    "            ],\n",
    "            \"responses\": [\n",
    "                \"ğŸ“… **Äáº¶T Lá»ŠCH KHÃM**\\n\\nTÃ´i sáº½ giÃºp báº¡n Ä‘áº·t lá»‹ch khÃ¡m. Vui lÃ²ng cung cáº¥p:\\n\\n1ï¸âƒ£ **ChuyÃªn khoa:** (tim máº¡ch, da liá»…u, nhi, tháº§n kinh...)\\n2ï¸âƒ£ **Thá»i gian:** (ngÃ y mai, thá»© 2, 15/12...)\\n3ï¸âƒ£ **Giá» Æ°u tiÃªn:** (sÃ¡ng, chiá»u, tá»‘i)\\n\\nğŸ’¡ **ChuyÃªn khoa cÃ³ sáºµn:**\\n- ğŸ«€ Tim máº¡ch\\n- ğŸ§  Tháº§n kinh\\n- ğŸ‘¶ Nhi khoa\\n- ğŸ¦´ XÆ°Æ¡ng khá»›p\\n- ğŸ‘ï¸ Máº¯t\\n- ğŸ‘‚ Tai mÅ©i há»ng\\n- ğŸ§´ Da liá»…u\\n\\nHÃ£y cho tÃ´i biáº¿t báº¡n muá»‘n khÃ¡m chuyÃªn khoa gÃ¬?\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"tag\": \"appointment_check\",\n",
    "            \"patterns\": [\n",
    "                \"kiá»ƒm tra lá»‹ch háº¹n\", \"check appointment\", \"xem lá»‹ch khÃ¡m\", \"tra lá»‹ch\",\n",
    "                \"lá»‹ch háº¹n cá»§a tÃ´i\", \"my appointments\", \"khi nÃ o tÃ´i khÃ¡m\", \"lá»‹ch khÃ¡m bá»‡nh\",\n",
    "                \"appointment status\", \"view appointments\", \"see my schedule\"\n",
    "            ],\n",
    "            \"responses\": [\n",
    "                \"ğŸ“‹ **KIá»‚M TRA Lá»ŠCH Háº¸N**\\n\\nÄá»ƒ tra cá»©u lá»‹ch háº¹n, tÃ´i cáº§n:\\n\\nğŸ†” **Sá»‘ Ä‘iá»‡n thoáº¡i** hoáº·c **MÃ£ bá»‡nh nhÃ¢n**\\nğŸ“§ **Email** Ä‘Äƒng kÃ½\\n\\nğŸ“± **Hoáº·c báº¡n cÃ³ thá»ƒ:**\\n- ÄÄƒng nháº­p vÃ o app\\n- Gá»i hotline: 1900-XXX-XXX\\n- Truy cáº­p website bá»‡nh viá»‡n\\n\\nğŸ’¡ **ThÃ´ng tin lá»‹ch háº¹n bao gá»“m:**\\n- NgÃ y giá» khÃ¡m\\n- BÃ¡c sÄ© khÃ¡m\\n- PhÃ²ng khÃ¡m\\n- Ghi chÃº Ä‘áº·c biá»‡t\\n\\nBáº¡n cÃ³ sá»‘ Ä‘iá»‡n thoáº¡i Ä‘Äƒng kÃ½ khÃ´ng?\"\n",
    "            ]\n",
    "        },\n",
    "        # EMERGENCY DETECTION\n",
    "        {\n",
    "            \"tag\": \"emergency\",\n",
    "            \"patterns\": [\n",
    "                \"cáº¥p cá»©u\", \"kháº©n cáº¥p\", \"emergency\", \"help me\", \"tÃ´i cáº§n cáº¥p cá»©u\",\n",
    "                \"Ä‘au tim\", \"heart attack\", \"khÃ³ thá»Ÿ\", \"difficulty breathing\", \"stroke\",\n",
    "                \"Ä‘á»™t quá»µ\", \"báº¥t tá»‰nh\", \"unconscious\", \"cháº£y mÃ¡u nhiá»u\", \"severe bleeding\",\n",
    "                \"ngá»™ Ä‘á»™c\", \"poisoning\", \"tai náº¡n\", \"accident\", \"gÃ£y xÆ°Æ¡ng\", \"broken bone\"\n",
    "            ],\n",
    "            \"responses\": [\n",
    "                \"ğŸš¨ **TÃŒNH HUá»NG KHáº¨N Cáº¤P**\\n\\n**HÃ€NH Äá»˜NG NGAY:**\\nğŸ“ **Gá»ŒI 115** (Cáº¥p cá»©u)\\nğŸ“ **Gá»ŒI 114** (Cáº£nh sÃ¡t)\\nğŸ“ **Gá»ŒI 113** (ChÃ¡y ná»•)\\n\\nğŸ¥ **Báº¤M Sá» HOTLINE:**\\n- Bá»‡nh viá»‡n Chá»£ Ráº«y: (028) 3855 4269\\n- Bá»‡nh viá»‡n Báº¡ch Mai: (024) 3869 3731\\n- Bá»‡nh viá»‡n E: (024) 3869 0736\\n\\nâš¡ **Cáº¤P Cá»©U Táº I CHá»–:**\\nâœ… Giá»¯ bÃ¬nh tÄ©nh\\nâœ… Äáº£m báº£o an toÃ n\\nâœ… SÆ¡ cá»©u cÆ¡ báº£n\\nâœ… Chá» xe cá»©u thÆ°Æ¡ng\\n\\n**KHÃ”NG Tá»° Ã DI CHUYá»‚N** náº¿u nghi ngá» cháº¥n thÆ°Æ¡ng cá»™t sá»‘ng!\"\n",
    "            ]\n",
    "        },\n",
    "        # HEALTH EDUCATION\n",
    "        {\n",
    "            \"tag\": \"health_tips\",\n",
    "            \"patterns\": [\n",
    "                \"lá»i khuyÃªn sá»©c khá»e\", \"health tips\", \"cÃ¡ch chÄƒm sÃ³c sá»©c khá»e\",\n",
    "                \"sá»‘ng khá»e\", \"healthy living\", \"phÃ²ng bá»‡nh\", \"prevention\",\n",
    "                \"tÄƒng cÆ°á»ng sá»©c khá»e\", \"boost immunity\", \"tips sá»©c khá»e\"\n",
    "            ],\n",
    "            \"responses\": [\n",
    "                \"ğŸ’¡ **Lá»œI KHUYÃŠN Sá»NG KHá»E**\\n\\nğŸ¥— **DINH DÆ¯á» NG:**\\n- Ä‚n 5 pháº§n rau cá»§/ngÃ y\\n- Uá»‘ng 2-3L nÆ°á»›c/ngÃ y\\n- Háº¡n cháº¿ Ä‘Æ°á»ng, muá»‘i\\n- Omega-3 tá»« cÃ¡ biá»ƒn\\n\\nğŸƒ **Váº¬N Äá»˜NG:**\\n- 150 phÃºt/tuáº§n cÆ°á»ng Ä‘á»™ vá»«a\\n- BÆ¡i lá»™i, Ä‘i bá»™, yoga\\n- Táº­p thá»ƒ dá»¥c buá»•i sÃ¡ng\\n\\nğŸ˜´ **NGHá»ˆ NGÆ I:**\\n- Ngá»§ 7-9 tiáº¿ng/Ä‘Ãªm\\n- Äi ngá»§ trÆ°á»›c 23h\\n- TrÃ¡nh mÃ n hÃ¬nh trÆ°á»›c ngá»§\\n\\nğŸ§˜ **TINH THáº¦N:**\\n- Thiá»n Ä‘á»‹nh, thÆ° giÃ£n\\n- Káº¿t ná»‘i xÃ£ há»™i\\n- Quáº£n lÃ½ stress\\n\\nğŸ¥ **KHÃM Äá»ŠNH Ká»²:**\\n- Tá»•ng quÃ¡t 6 thÃ¡ng/láº§n\\n- XÃ©t nghiá»‡m mÃ¡u hÃ ng nÄƒm\\n- Táº§m soÃ¡t ung thÆ°\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"tag\": \"medicine_info\",\n",
    "            \"patterns\": [\n",
    "                \"thÃ´ng tin thuá»‘c\", \"medicine information\", \"cÃ¡ch dÃ¹ng thuá»‘c\",\n",
    "                \"tÃ¡c dá»¥ng phá»¥\", \"side effects\", \"liá»u dÃ¹ng\", \"dosage\",\n",
    "                \"thuá»‘c an toÃ n\", \"drug safety\", \"tÆ°Æ¡ng tÃ¡c thuá»‘c\"\n",
    "            ],\n",
    "            \"responses\": [\n",
    "                \"ğŸ’Š **THÃ”NG TIN THUá»C**\\n\\nâš ï¸ **NGUYÃŠN Táº®C AN TOÃ€N:**\\n- Chá»‰ dÃ¹ng theo Ä‘Æ¡n bÃ¡c sÄ©\\n- Äá»c ká»¹ hÆ°á»›ng dáº«n\\n- KhÃ´ng tá»± Ã½ tÄƒng liá»u\\n- BÃ¡o bÃ¡c sÄ© vá» dá»‹ á»©ng\\n\\nğŸ“‹ **THÃ”NG TIN Cáº¦N BIáº¾T:**\\n- TÃªn thuá»‘c (hoáº¡t cháº¥t)\\n- Liá»u lÆ°á»£ng & cÃ¡ch dÃ¹ng\\n- Thá»i gian Ä‘iá»u trá»‹\\n- TÃ¡c dá»¥ng phá»¥\\n- TÆ°Æ¡ng tÃ¡c thuá»‘c\\n\\nğŸ” **TRA Cá»¨U THUá»C:**\\n- Website Cá»¥c DÆ°á»£c\\n- App thuá»‘c chÃ­nh thá»©c\\n- Há»i dÆ°á»£c sÄ©\\n\\nâš¡ **KHáº¨N Cáº¤P:** Gá»i 115 náº¿u nghi ngá»™ Ä‘á»™c thuá»‘c!\\n\\nBáº¡n cáº§n thÃ´ng tin thuá»‘c cá»¥ thá»ƒ nÃ o?\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"âœ… Created knowledge base with {len(healthcare_knowledge_base['intents'])} intents\")\n",
    "print(\"ğŸ“‹ Available intents:\", [intent['tag'] for intent in healthcare_knowledge_base['intents']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
